import os
import numpy as np
import pandas as pd
import torch
import torchaudio
import torchaudio.transforms as T
import soundfile as sf
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import joblib

class SpeakerListenerDataset(Dataset):
    def __init__(self, mapping_csv, stride=10, sequence_length=100, normalize=True, scaler_path="scaler.pkl", audio_scaler_path="audio_scaler.pkl"):
        
        self.target_length = 750  # âœ… ç»Ÿä¸€é•¿åº¦ä¸º 750 å¸§
        self.mapping_df = pd.read_csv(mapping_csv, engine="c")  # âœ… ä½¿ç”¨ C è§£æå™¨åŠ é€Ÿ
        self.sequence_length = sequence_length
        self.stride = stride
        self.normalize = normalize

        # **ğŸ”¹ åŠ è½½å…¨å±€å½’ä¸€åŒ–å‚æ•°**

        self.scaler = joblib.load(scaler_path)  
        # **åŠ è½½éŸ³é¢‘ scaler**
        self.mfcc_scaler, self.mel_scaler = joblib.load(audio_scaler_path)  


    def _convert_path(self, path, ext):
        """è‡ªåŠ¨é€‚é… Windows å’Œ Ubuntu è·¯å¾„ï¼ŒåŒæ—¶æ›¿æ¢æ‰©å±•å"""
        path = "Robot_dataset/Face/"+path+"."+ext
        return path
    
    def _convert_audio_path(self, path,ext):
        path = "Robot_dataset/Audio_files/"+path+"."+ext
        return path
    def _interpolate_missing_frames(self, df):
        """å¡«å……ç¼ºå¤±å¸§ï¼Œç¡®ä¿ 750 å¸§"""
        if "frame" not in df.columns:
            return df  # è‹¥æ—  frame åˆ—ï¼Œåˆ™ç›´æ¥è¿”å›
        if len(df) == self.target_length:
            return df

        full_frame_range = np.arange(0, self.target_length)  # ç”Ÿæˆ 0~749 å¸§
        df = df.set_index("frame").reindex(full_frame_range)  # é‡æ–°ç´¢å¼•
        df.interpolate(method="linear", inplace=True)  # çº¿æ€§æ’å€¼
        df.ffill(inplace=True)  # å‘å‰å¡«å……
        df.bfill(inplace=True)  # å‘åå¡«å……

        return df.reset_index()

    def _load_parquet(self, parquet_path):
        """è¯»å– Parquet å¹¶è¿›è¡Œç‰¹å¾ç­›é€‰ + å½’ä¸€åŒ– + æ’å€¼è¡¥å¸§"""
        if not os.path.exists(parquet_path) or os.stat(parquet_path).st_size == 0:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {parquet_path}")
            return None

        try:
            df = pd.read_parquet(parquet_path)  # è¯»å– Parquet
            
            df = df.loc[:self.target_length - 1]  # âœ… é™åˆ¶ä¸º 750 å¸§
            df = self._interpolate_missing_frames(df)  # **âœ… æ’å€¼è¡¥å¸§**
            df = df.drop(columns=["frame"])  # åˆ é™¤ frame åˆ—å¤šä½™åˆ—
            print("âœ… åŠ è½½æˆåŠŸ:", parquet_path)
            # **ä½¿ç”¨å…¨å±€ scaler å½’ä¸€åŒ–**    
            df = self.scaler.transform(df.to_numpy())  # âœ… ä½¿ç”¨å…¨å±€ scaler å½’ä¸€åŒ–

            return df
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥: {parquet_path} - {e}")
            return None


    def _load_audio(self, audio_path):
        """ä½¿ç”¨ torchaudio è¯»å– WAV éŸ³é¢‘å¹¶æå– MFCC + Î” + Mel é¢‘è°±ç‰¹å¾"""
        if not os.path.exists(audio_path):
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return None, None

        try:
            # 1ï¸âƒ£ **è¯»å–éŸ³é¢‘**
            waveform, sr = torchaudio.load(audio_path)  # è¯»å–éŸ³é¢‘
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)  # è½¬æ¢ä¸ºå•å£°é“

            # 2ï¸âƒ£ **è®¡ç®— hop_length ç¡®ä¿éŸ³é¢‘å’Œè§†é¢‘å¸§ç‡å¯¹é½**
            fps = 25  # è§†é¢‘å¸§ç‡ 25 FPS
            hop_length = int(sr / fps)  # è®¡ç®— hop_length
            target_frames = 750  # ç¡®ä¿ç‰¹å¾ä¸è§†é¢‘å¯¹é½

            # 3ï¸âƒ£ **è®¡ç®— Mel é¢‘è°±**
            mel_transform = T.MelSpectrogram(sample_rate=sr, n_mels=40, n_fft=1024, hop_length=hop_length)
            mel_spec = mel_transform(waveform).squeeze(0).numpy()  # shape: (40, æ—¶é—´å¸§æ•°)
            #mel_spec = np.log1p(mel_spec)
            # 4ï¸âƒ£ **è®¡ç®— MFCC**
            mfcc_transform = T.MFCC(sample_rate=sr, n_mfcc=13, melkwargs={"n_mels": 40, "n_fft": 1024, "hop_length": hop_length})
            mfcc = mfcc_transform(waveform).squeeze(0).numpy()  # shape: (13, æ—¶é—´å¸§æ•°)

            # 5ï¸âƒ£ **è®¡ç®— MFCC Î”ï¼ˆé€Ÿåº¦ï¼‰å’Œ Î”Î”ï¼ˆåŠ é€Ÿåº¦ï¼‰**
            mfcc_delta = np.gradient(mfcc, axis=1)
            mfcc_delta2 = np.gradient(mfcc_delta, axis=1)
            mfccs_all = np.stack((mfcc, mfcc_delta, mfcc_delta2), axis=0)  # shape: (3, 13, æ—¶é—´å¸§æ•°)

            # 6ï¸âƒ£ **ç¡®ä¿éŸ³é¢‘å¸§æ•° >= 750**
            current_frames = mfccs_all.shape[-1]
            if current_frames < target_frames:
                pad_width = target_frames - current_frames
                mfccs_all = np.pad(mfccs_all, ((0, 0), (0, 0), (0, pad_width)), mode="edge")
                mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode="edge")
            elif current_frames > target_frames:
                mfccs_all = mfccs_all[:, :, :target_frames]  # æˆªæ–­
                mel_spec = mel_spec[:, :target_frames]  # æˆªæ–­

            # 7ï¸âƒ£ **å½’ä¸€åŒ–**
            mfccs_all = mfccs_all.transpose(2, 0, 1).reshape(target_frames, -1)
            mfccs_all = self.mfcc_scaler.transform(mfccs_all)
            
            mel_spec = mel_spec.T
            mel_spec = self.mel_scaler.transform(mel_spec)
            
            return mfccs_all, mel_spec  # å½’ä¸€åŒ–åçš„ MFCC å’Œ Mel é¢‘è°±
        except Exception as e:
            print(f"âš ï¸  éŸ³é¢‘åŠ è½½å¤±è´¥: {audio_path} - {e}")
            return None, None


    def _create_sequences(self, expr_array, mfcc, mel):
        
        num_sequences = (expr_array.shape[0] - self.sequence_length) // self.stride + 1
        
        expr_sequences = np.zeros((num_sequences, self.sequence_length, expr_array.shape[1]), dtype=np.float32)
        mfcc_sequences = np.zeros((num_sequences, self.sequence_length, mfcc.shape[1]), dtype=np.float32)
        mel_sequences = np.zeros((num_sequences, self.sequence_length, mel.shape[1]), dtype=np.float32)

        for i in range(num_sequences):
            start_idx = i * self.stride
            end_idx = start_idx + self.sequence_length

            expr_sequences[i] = expr_array[start_idx:end_idx]  # (sequence_length, 161)
            mfcc_sequences[i] = mfcc[start_idx:end_idx]  # (sequence_length, 39)
            mel_sequences[i] = mel[start_idx:end_idx]  # (sequence_length, 40)

        return expr_sequences, mfcc_sequences, mel_sequences


    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.mapping_df)

    def __getitem__(self, idx):
        """æŒ‰ç´¢å¼•è¯»å–æ•°æ®ï¼ˆLazy Loadingï¼‰"""
        row = self.mapping_df.iloc[idx]
        speaker_parquet = self._convert_path(row.iloc[1], "parquet")
        listener_parquet = self._convert_path(row.iloc[2], "parquet")
        speaker_audio = self._convert_audio_path(row.iloc[1], "wav")
        listener_audio = self._convert_audio_path(row.iloc[2], "wav")

        speaker_df = self._load_parquet(speaker_parquet)
        listener_df = self._load_parquet(listener_parquet)
        speaker_mfcc, speaker_mel = self._load_audio(speaker_audio)
        listener_mfcc, listener_mel = self._load_audio(listener_audio)

        if any(x is None for x in [speaker_df, listener_df, speaker_mfcc, listener_mel, listener_mfcc, listener_mel]):
            print(f"âš ï¸  è·³è¿‡ç´¢å¼• {idx}: {speaker_parquet} æˆ– {listener_parquet} åŠ è½½å¤±è´¥")
            return None

        speaker_sequences_expr, speaker_sequences_mfcc, speaker_sequences_mel = self._create_sequences(speaker_df, speaker_mfcc, speaker_mel)
        
        listener_sequences_expr, listener_sequences_mfcc, listener_sequences_mel = self._create_sequences(listener_df, listener_mfcc, listener_mel)

        return speaker_sequences_expr, listener_sequences_expr, speaker_sequences_mfcc, listener_sequences_mfcc, speaker_sequences_mel, listener_sequences_mel


def get_dataloader(mapping_csv, batch_size=2, stride=10, num_workers=0, sequence_length=100, scaler_path="scaler.pkl", audio_scaler_path="audio_scaler.pkl"):
    dataset = SpeakerListenerDataset(mapping_csv, stride, sequence_length, scaler_path=scaler_path, audio_scaler_path=audio_scaler_path)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)


if __name__ == '__main__':
    # âœ… æŒ‡å®šæµ‹è¯•è·¯å¾„
    mapping_csv = "Robot_dataset/train.csv"
    face_scaler_path = "Robot_face_gen/data_process/Face_Scaler.pkl"
    audio_scaler_path = "Robot_face_gen/data_process/Audio_Scaler.pkl"
    # âœ… åˆ›å»º DataLoader
    dataloader = get_dataloader(mapping_csv, batch_size=2, stride=10, num_workers=0, sequence_length=100, scaler_path=face_scaler_path,audio_scaler_path = audio_scaler_path)

    # âœ… å–å‡ºä¸€ä¸ª batch å¹¶æ£€æŸ¥å½¢çŠ¶
    for i, (speaker_seq, listener_seq, speaker_mfcc, listener_mfcc, speaker_mel, listener_mel) in enumerate(dataloader):
        print(f"ğŸŸ¢ Batch {i} Loaded Successfully!")

        # **æ£€æŸ¥è¡¨æƒ…æ•°æ® Shape**
        print(f"ğŸ—¿ Speaker Facial Expression Shape: {speaker_seq.shape}")  
        print(f"ğŸ—¿ Listener Facial Expression Shape: {listener_seq.shape}")

        # **æ£€æŸ¥éŸ³é¢‘ MFCC + Î” + Î”Î” ç‰¹å¾ Shape**
        print(f"ğŸµ Speaker MFCC Shape: {speaker_mfcc.shape}")  
        print(f"ğŸµ Listener MFCC Shape: {listener_mfcc.shape}")

        # **æ£€æŸ¥ Mel é¢‘è°± Shape**
        print(f"ğŸµ Speaker Mel Spectrogram Shape: {speaker_mel.shape}")  
        print(f"ğŸµ Listener Mel Spectrogram Shape: {listener_mel.shape}")

        # **åªæµ‹è¯•ä¸€ä¸ª batch**
        break
